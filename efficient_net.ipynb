{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0RBPy2mb16T"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import EfficientNetB4,EfficientNetB5,EfficientNetB6,EfficientNetB7\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from zipfile import ZipFile\n",
        "with ZipFile('/content/New_Dataset.zip', 'r') as zipObj:\n",
        "\n",
        "   zipObj.extractall('IMAGES')"
      ],
      "metadata": {
        "id": "2T2--AMMdDYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p = Augmentor.Pipeline(\"/content/IMAGES/images\",output_directory=\"/content/output\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8DskcYgdKhy",
        "outputId": "afd46613-8bbd-428c-bdb6-3cd7cae919c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialised with 210 image(s) found.\n",
            "Output directory set to /content/output."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p.rotate(probability=0.7, max_left_rotation=10, max_right_rotation=10)\n",
        "\n",
        "p.zoom(probability=0.3, min_factor=1.1, max_factor=1.6)"
      ],
      "metadata": {
        "id": "RskZLhEydQLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p.sample(1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMeKdqg6dTSy",
        "outputId": "de1533b6-8c66-4623-a493-329efbe1d65d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing <PIL.Image.Image image mode=RGB size=416x416 at 0x78A3C06761A0>: 100%|██████████| 1000/1000 [00:38<00:00, 25.70 Samples/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "data_dir = pathlib.Path('/content/IMAGES/New_Dataset')\n",
        "'''img_height=224\n",
        "img_width=224'''"
      ],
      "metadata": {
        "id": "6L0jdoZAdWj7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "50afe23d-e9b8-43cb-c478-7440882b15f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'img_height=224\\nimg_width=224'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    data_dir,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=123,\n",
        "    image_size=(380, 380),\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    data_dir,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=123,\n",
        "    image_size=(380, 380),\n",
        "    batch_size=32\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPCVdrzqdacz",
        "outputId": "fa975135-512b-43ac-a8bd-3f55124ba2ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1262 files belonging to 2 classes.\n",
            "Using 1010 files for training.\n",
            "Found 1262 files belonging to 2 classes.\n",
            "Using 252 files for validation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers.experimental.preprocessing import Rescaling"
      ],
      "metadata": {
        "id": "qBHA-NYrdjF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalization_layer = Rescaling(1./255)\n",
        "train_ds_normalized = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
        "val_ds_normalized = val_ds.map(lambda x, y: (normalization_layer(x), y))"
      ],
      "metadata": {
        "id": "cUXNVlvndmYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes=2\n",
        "def one_hot_encoding(x, y):\n",
        "    return x, tf.one_hot(y, depth=num_classes)\n",
        "\n",
        "train_ds_encoded = train_ds_normalized.map(one_hot_encoding)\n",
        "val_ds_encoded = val_ds_normalized.map(one_hot_encoding)"
      ],
      "metadata": {
        "id": "Thxr5wuQdphP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds_encoded = train_ds_encoded.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "val_ds_encoded = val_ds_encoded.prefetch(buffer_size=tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "0h_L5jtBdxeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "model_filepath = \"/content/drive/My Drive/model-{epoch:02d}-{val_accuracy:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(\n",
        "    filepath=model_filepath,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "iLlbREokd1Bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model_b4 = EfficientNetB4(weights='imagenet', include_top=False, input_shape=(380, 380, 3))\n",
        "base_model_b5 = EfficientNetB5(weights='imagenet', include_top=False, input_shape=(456, 456, 3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5HXQZ0qfOh3",
        "outputId": "791be412-2657-49e0-fd58-bef278da5f19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb4_notop.h5\n",
            "71686520/71686520 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb5_notop.h5\n",
            "115263384/115263384 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dropout"
      ],
      "metadata": {
        "id": "pf6hDurzgavO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_b4 = base_model_b4.output\n",
        "x_b4 = GlobalAveragePooling2D()(x_b4)\n",
        "x_b4 = Dense(1024, activation='relu')(x_b4)\n",
        "x_b4 = Dropout(0.20)(x_b4)\n",
        "x_b4 = Dense(512, activation='relu')(x_b4)\n",
        "x_b4 = Dense(256, activation='relu')(x_b4)\n",
        "x_b4 = Dense(64, activation='relu')(x_b4)"
      ],
      "metadata": {
        "id": "N2u77jzYfYgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_b4 = Dense(num_classes, activation='softmax')(x_b4)\n",
        "model_efficientnet_b4 = Model(inputs=base_model_b4.input, outputs=output_b4)\n",
        "model_efficientnet_b4.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Ad3HK8vEExgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####EficientNetB5\n",
        "x_b5 = base_model_b5.output\n",
        "x_b5 = GlobalAveragePooling2D()(x_b5)\n",
        "x_b5 = Dense(1024, activation='relu')(x_b5)\n",
        "x_b5 = Dropout(0.20)(x_b5)\n",
        "x_b5 = Dense(512, activation='relu')(x_b5)\n",
        "#x_b5 = Dense(0.25)(x_b5)\n",
        "x_b5 = Dense(256, activation='relu')(x_b5)\n",
        "x_b5 = Dense(64, activation='relu')(x_b5)\n",
        "x_b5 = Dense(16, activation='relu')(x_b5)\n",
        "output_b5 = Dense(num_classes, activation='softmax')(x_b5)\n",
        "model_efficientnet_b5 = Model(inputs=base_model_b5.input, outputs=output_b5)\n",
        "model_efficientnet_b5.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "r0SrXWWDgtbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_efficientnet_b5.fit(\n",
        "    train_ds_encoded,\n",
        "    validation_data=val_ds_encoded,\n",
        "    epochs=100,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "FwiRYpLkj_1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_label = np.concatenate([y for x, y in val_ds_normalized], axis=0)\n",
        "print(test_label.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Aabuylv7fN3",
        "outputId": "8818abf6-3459-4dda-c24e-ea7d8aec4310"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(252,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "Y_pred = model_efficientnet_b5.predict(val_ds_normalized)\n",
        "y_pred = np.argmax(Y_pred, axis=1)\n",
        "print('Confusion Matrix')\n",
        "cm=confusion_matrix(test_label, y_pred)\n",
        "print(cm)\n",
        "print('Classification Report')\n",
        "target_names = ['Pothole','SpeedBreaker']\n",
        "print(classification_report(test_label, y_pred, target_names=target_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcI6F0TB73hT",
        "outputId": "b6cfd23d-6edc-4eb8-acd5-415096ef4114"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 2s 199ms/step\n",
            "Confusion Matrix\n",
            "[[  0 143]\n",
            " [  0 109]]\n",
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Pothole       0.00      0.00      0.00       143\n",
            "SpeedBreaker       0.43      1.00      0.60       109\n",
            "\n",
            "    accuracy                           0.43       252\n",
            "   macro avg       0.22      0.50      0.30       252\n",
            "weighted avg       0.19      0.43      0.26       252\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c_K2_VpOjCg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Without Customization Accuracy was 0.61"
      ],
      "metadata": {
        "id": "1aM468Tr8hfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_efficientnet_b4.fit(\n",
        "    train_ds_encoded,\n",
        "    validation_data=val_ds_encoded,\n",
        "    epochs=25,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "WJhmE6x_V7JM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "Y_pred = model_efficientnet_b4.predict(val_ds)\n",
        "y_pred = np.argmax(Y_pred, axis=1)\n",
        "print('Confusion Matrix')\n",
        "cm=confusion_matrix(test_label, y_pred)\n",
        "print(cm)\n",
        "print('Classification Report')\n",
        "target_names = ['Pothole','SpeedBreaker']\n",
        "print(classification_report(test_label, y_pred, target_names=target_names))"
      ],
      "metadata": {
        "id": "VAPErjuJWDkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''model_efficientnet_b6.fit(\n",
        "    train_ds_encoded,\n",
        "    validation_data=val_ds_encoded,\n",
        "    epochs=100,\n",
        "    verbose=1\n",
        ")'''"
      ],
      "metadata": {
        "id": "asGeFUG3WBw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "data_dir = pathlib.Path('/content/IMAGES/New_Dataset')\n",
        "\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    data_dir,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=123,\n",
        "    image_size=(380, 380),  # Change the image size to match the new size\n",
        "    batch_size=16\n",
        ")\n",
        "\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    data_dir,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=123,\n",
        "    image_size=(380, 380),  # Change the image size to match the new size\n",
        "    batch_size=16\n",
        ")\n",
        "\n",
        "normalization_layer = Rescaling(1./255)\n",
        "train_ds_normalized = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
        "val_ds_normalized = val_ds.map(lambda x, y: (normalization_layer(x), y))\n",
        "\n",
        "num_classes = 2\n",
        "def one_hot_encoding(x, y):\n",
        "    return x, tf.one_hot(y, depth=num_classes)\n",
        "\n",
        "train_ds_encoded = train_ds_normalized.map(one_hot_encoding)\n",
        "val_ds_encoded = val_ds_normalized.map(one_hot_encoding)\n",
        "\n",
        "#train_ds_encoded = train_ds_encoded.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "#val_ds_encoded = val_ds_encoded.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "train_ds_encoded = train_ds_encoded.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "val_ds_encoded = val_ds_encoded.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "base_model_b4 = EfficientNetB4(weights='imagenet', include_top=False, input_shape=(380, 380, 3))  # Change the input shape here\n",
        "\n",
        "x_b4 = base_model_b4.output\n",
        "x_b4 = GlobalAveragePooling2D()(x_b4)\n",
        "#x_b4 = Dense(1024, activation='relu')(x_b4)\n",
        "#x_b4 = Dropout(0.20)(x_b4)\n",
        "x_b4 = Dense(512, activation='relu')(x_b4)\n",
        "x_b4 = Dropout(0.20)(x_b4)\n",
        "x_b4 = Dense(256, activation='relu')(x_b4)\n",
        "\n",
        "output_b4 = Dense(num_classes, activation='softmax')(x_b4)  # Change the number of units here\n",
        "model_efficientnet_b4 = Model(inputs=base_model_b4.input, outputs=output_b4)\n",
        "model_efficientnet_b4.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model_efficientnet_b4.fit(\n",
        "    train_ds_encoded,\n",
        "    validation_data=val_ds_encoded,\n",
        "    epochs=25,\n",
        "    verbose=1\n",
        ")\n"
      ],
      "metadata": {
        "id": "MNjqh0ErAdfA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}